{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality Forecasting - Complete Workflow\n",
    "\n",
    "This notebook demonstrates the complete workflow for short-term air quality forecasting using satellite and reanalysis data.\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Forecast ground-level O3 and NO2 concentrations 24-48 hours ahead\n",
    "- **Data**: Meteorological forecasts, satellite observations, and ground truth measurements\n",
    "- **Models**: Random Forest, XGBoost, LSTM, CNN-LSTM\n",
    "- **Target**: Hourly predictions with confidence intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root / 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from data_preprocessing.data_loader import DataLoader\n",
    "from feature_engineering.feature_engineer import FeatureEngineer\n",
    "from models.model_trainer import ModelTrainer\n",
    "from evaluation.model_evaluator import ModelEvaluator\n",
    "from forecasting.forecaster import AirQualityForecaster\n",
    "from utils.helpers import ConfigManager, LoggingUtils, DataUtils\n",
    "\n",
    "print(\"Project modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = project_root / 'configs' / 'config.yaml'\n",
    "config = ConfigManager.load_config(str(config_path))\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"- Project: {config.get('project', {}).get('name', 'N/A')}\")\n",
    "print(f\"- Version: {config.get('project', {}).get('version', 'N/A')}\")\n",
    "print(f\"- Data path: {config.get('data', {}).get('raw_data_path', 'N/A')}\")\n",
    "print(f\"- Models: {list(config.get('models', {}).keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logger = LoggingUtils.setup_logger(\n",
    "    name=\"air_quality_forecast\",\n",
    "    level=\"INFO\",\n",
    "    log_file=str(project_root / \"logs\" / \"training.log\")\n",
    ")\n",
    "\n",
    "logger.info(\"Starting air quality forecasting workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader(config_path=str(config_path))\n",
    "\n",
    "# Load site coordinates\n",
    "coords_df = data_loader.load_site_coordinates()\n",
    "print(\"Site Coordinates:\")\n",
    "display(coords_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data for all sites\n",
    "train_data_dict = data_loader.load_training_data()\n",
    "print(f\"Loaded training data for {len(train_data_dict)} sites\")\n",
    "\n",
    "# Display data summary for each site\n",
    "for site_id, data in train_data_dict.items():\n",
    "    summary = data_loader.get_data_summary(data)\n",
    "    print(f\"\\nSite {site_id}:\")\n",
    "    print(f\"  Shape: {summary['shape']}\")\n",
    "    print(f\"  Date range: {summary['date_range']['start']} to {summary['date_range']['end']}\")\n",
    "    print(f\"  Duration: {summary['date_range']['duration_days']} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine data structure for Site 1\n",
    "site_1_data = train_data_dict[1]\n",
    "print(\"Sample data structure (Site 1):\")\n",
    "print(f\"Columns: {list(site_1_data.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(site_1_data.head())\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "display(site_1_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data availability and missing values\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot target variables time series for Site 1\n",
    "axes[0, 0].plot(site_1_data['datetime'], site_1_data['O3_target'], alpha=0.7, label='O3')\n",
    "axes[0, 0].set_title('Ground-level O3 (Site 1)')\n",
    "axes[0, 0].set_ylabel('O3 (μg/m³)')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "axes[0, 1].plot(site_1_data['datetime'], site_1_data['NO2_target'], alpha=0.7, label='NO2', color='orange')\n",
    "axes[0, 1].set_title('Ground-level NO2 (Site 1)')\n",
    "axes[0, 1].set_ylabel('NO2 (μg/m³)')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Missing data heatmap\n",
    "missing_data = site_1_data.isnull().sum()\n",
    "missing_pct = (missing_data / len(site_1_data) * 100).sort_values(ascending=False)\n",
    "missing_pct[missing_pct > 0].plot(kind='bar', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Missing Data Percentage (Site 1)')\n",
    "axes[1, 0].set_ylabel('% Missing')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Correlation matrix of key variables\n",
    "key_vars = ['O3_target', 'NO2_target', 'T_forecast', 'q_forecast', 'O3_forecast', 'NO2_forecast']\n",
    "corr_matrix = site_1_data[key_vars].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Correlation Matrix (Site 1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess data for all sites\n",
    "cleaned_data_dict = {}\n",
    "\n",
    "for site_id, data in train_data_dict.items():\n",
    "    print(f\"Cleaning data for Site {site_id}...\")\n",
    "    \n",
    "    # Clean the data\n",
    "    cleaned_data = data_loader.clean_data(data)\n",
    "    \n",
    "    # Prepare features and targets\n",
    "    features_df, targets_df = data_loader.prepare_features_targets(cleaned_data)\n",
    "    \n",
    "    # Combine for storage\n",
    "    combined_df = pd.concat([features_df, targets_df], axis=1)\n",
    "    combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]  # Remove duplicate columns\n",
    "    \n",
    "    cleaned_data_dict[site_id] = combined_df\n",
    "    \n",
    "    print(f\"  Original shape: {data.shape}\")\n",
    "    print(f\"  Cleaned shape: {cleaned_data.shape}\")\n",
    "    print(f\"  Final shape: {combined_df.shape}\")\n",
    "\n",
    "print(\"\\nData cleaning completed for all sites.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "feature_engineer = FeatureEngineer(config_path=str(config_path))\n",
    "\n",
    "# Apply feature engineering to all sites\n",
    "engineered_data_dict = {}\n",
    "\n",
    "for site_id, data in cleaned_data_dict.items():\n",
    "    print(f\"Engineering features for Site {site_id}...\")\n",
    "    \n",
    "    # Apply comprehensive feature engineering\n",
    "    engineered_data = feature_engineer.create_all_features(data)\n",
    "    engineered_data_dict[site_id] = engineered_data\n",
    "    \n",
    "    print(f\"  Original features: {data.shape[1]}\")\n",
    "    print(f\"  Engineered features: {engineered_data.shape[1]}\")\n",
    "    print(f\"  New features added: {engineered_data.shape[1] - data.shape[1]}\")\n",
    "\n",
    "print(\"\\nFeature engineering completed for all sites.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature groups\n",
    "feature_groups = feature_engineer.get_feature_importance_groups()\n",
    "print(\"Feature Groups:\")\n",
    "for group_name, features in feature_groups.items():\n",
    "    if isinstance(features, list):\n",
    "        print(f\"  {group_name}: {len(features)} features\")\n",
    "    else:\n",
    "        # Pattern-based features\n",
    "        pattern_features = [col for col in engineered_data_dict[1].columns if features in col]\n",
    "        print(f\"  {group_name}: {len(pattern_features)} features (pattern: '{features}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all sites for model training\n",
    "all_sites_data = data_loader.combine_all_sites(engineered_data_dict)\n",
    "print(f\"Combined dataset shape: {all_sites_data.shape}\")\n",
    "print(f\"Date range: {all_sites_data['datetime'].min()} to {all_sites_data['datetime'].max()}\")\n",
    "print(f\"Sites included: {sorted(all_sites_data['site_id'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model trainer\n",
    "model_trainer = ModelTrainer(config_path=str(config_path))\n",
    "\n",
    "# Prepare data for O3 prediction\n",
    "print(\"Preparing data for O3 model training...\")\n",
    "X_o3, y_o3, feature_names = model_trainer.prepare_training_data(\n",
    "    all_sites_data, \n",
    "    all_sites_data[['O3_target', 'datetime', 'site_id']], \n",
    "    'O3_target'\n",
    ")\n",
    "\n",
    "print(f\"O3 training data prepared:\")\n",
    "print(f\"  Features shape: {X_o3.shape}\")\n",
    "print(f\"  Targets shape: {y_o3.shape}\")\n",
    "print(f\"  Number of features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "data_splits_o3 = model_trainer.split_data(X_o3, y_o3)\n",
    "\n",
    "print(\"Data splits for O3:\")\n",
    "for split_name, data in data_splits_o3.items():\n",
    "    print(f\"  {split_name}: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models for O3\n",
    "print(\"Training all models for O3 prediction...\")\n",
    "o3_models = model_trainer.train_all_models(\n",
    "    data_splits_o3['X_train'], data_splits_o3['y_train'],\n",
    "    data_splits_o3['X_val'], data_splits_o3['y_val']\n",
    ")\n",
    "\n",
    "print(f\"\\nTrained {len(o3_models)} models for O3:\")\n",
    "for model_name in o3_models.keys():\n",
    "    print(f\"  - {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for NO2\n",
    "print(\"Preparing data for NO2 model training...\")\n",
    "X_no2, y_no2, _ = model_trainer.prepare_training_data(\n",
    "    all_sites_data, \n",
    "    all_sites_data[['NO2_target', 'datetime', 'site_id']], \n",
    "    'NO2_target'\n",
    ")\n",
    "\n",
    "data_splits_no2 = model_trainer.split_data(X_no2, y_no2)\n",
    "\n",
    "print(\"Training all models for NO2 prediction...\")\n",
    "no2_models = model_trainer.train_all_models(\n",
    "    data_splits_no2['X_train'], data_splits_no2['y_train'],\n",
    "    data_splits_no2['X_val'], data_splits_no2['y_val']\n",
    ")\n",
    "\n",
    "print(f\"\\nTrained {len(no2_models)} models for NO2:\")\n",
    "for model_name in no2_models.keys():\n",
    "    print(f\"  - {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model evaluator\n",
    "evaluator = ModelEvaluator(config_path=str(config_path))\n",
    "\n",
    "# Evaluate O3 models\n",
    "print(\"Evaluating O3 models...\")\n",
    "o3_comparison = evaluator.compare_models(\n",
    "    o3_models, \n",
    "    data_splits_o3['X_test'], \n",
    "    data_splits_o3['y_test'], \n",
    "    target_name=\"O3\"\n",
    ")\n",
    "\n",
    "print(\"\\nO3 Model Comparison Results:\")\n",
    "display(o3_comparison[['model_name', 'r2', 'rmse', 'mae', 'correlation']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate NO2 models\n",
    "print(\"Evaluating NO2 models...\")\n",
    "no2_comparison = evaluator.compare_models(\n",
    "    no2_models, \n",
    "    data_splits_no2['X_test'], \n",
    "    data_splits_no2['y_test'], \n",
    "    target_name=\"NO2\"\n",
    ")\n",
    "\n",
    "print(\"\\nNO2 Model Comparison Results:\")\n",
    "display(no2_comparison[['model_name', 'r2', 'rmse', 'mae', 'correlation']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# O3 model comparison\n",
    "o3_comparison.plot(x='model_name', y='r2', kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('O3 Model Performance (R²)')\n",
    "axes[0].set_ylabel('R² Score')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# NO2 model comparison\n",
    "no2_comparison.plot(x='model_name', y='r2', kind='bar', ax=axes[1], color='lightcoral')\n",
    "axes[1].set_title('NO2 Model Performance (R²)')\n",
    "axes[1].set_ylabel('R² Score')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed plots for best performing models\n",
    "best_o3_model_name = o3_comparison.iloc[0]['model_name']\n",
    "best_no2_model_name = no2_comparison.iloc[0]['model_name']\n",
    "\n",
    "print(f\"Best O3 model: {best_o3_model_name}\")\n",
    "print(f\"Best NO2 model: {best_no2_model_name}\")\n",
    "\n",
    "# Get predictions from best models\n",
    "best_o3_model = o3_models[best_o3_model_name]\n",
    "best_no2_model = no2_models[best_no2_model_name]\n",
    "\n",
    "# Generate predictions\n",
    "is_o3_sequence = 'lstm' in best_o3_model_name.lower()\n",
    "is_no2_sequence = 'lstm' in best_no2_model_name.lower()\n",
    "\n",
    "if is_o3_sequence:\n",
    "    X_test_seq, y_test_seq = evaluator._prepare_sequence_data(\n",
    "        data_splits_o3['X_test'], data_splits_o3['y_test'], 24\n",
    "    )\n",
    "    o3_predictions = best_o3_model.predict(X_test_seq).flatten()\n",
    "    y_true_o3 = y_test_seq\n",
    "else:\n",
    "    o3_predictions = best_o3_model.predict(data_splits_o3['X_test'])\n",
    "    y_true_o3 = data_splits_o3['y_test']\n",
    "\n",
    "if is_no2_sequence:\n",
    "    X_test_seq, y_test_seq = evaluator._prepare_sequence_data(\n",
    "        data_splits_no2['X_test'], data_splits_no2['y_test'], 24\n",
    "    )\n",
    "    no2_predictions = best_no2_model.predict(X_test_seq).flatten()\n",
    "    y_true_no2 = y_test_seq\n",
    "else:\n",
    "    no2_predictions = best_no2_model.predict(data_splits_no2['X_test'])\n",
    "    y_true_no2 = data_splits_no2['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual for best models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# O3 predictions vs actual\n",
    "axes[0].scatter(y_true_o3, o3_predictions, alpha=0.6, s=30)\n",
    "min_val = min(np.min(y_true_o3), np.min(o3_predictions))\n",
    "max_val = max(np.max(y_true_o3), np.max(o3_predictions))\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual O3')\n",
    "axes[0].set_ylabel('Predicted O3')\n",
    "axes[0].set_title(f'O3 Predictions vs Actual ({best_o3_model_name})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# NO2 predictions vs actual\n",
    "axes[1].scatter(y_true_no2, no2_predictions, alpha=0.6, s=30, color='orange')\n",
    "min_val = min(np.min(y_true_no2), np.min(no2_predictions))\n",
    "max_val = max(np.max(y_true_no2), np.max(no2_predictions))\n",
    "axes[1].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual NO2')\n",
    "axes[1].set_ylabel('Predicted NO2')\n",
    "axes[1].set_title(f'NO2 Predictions vs Actual ({best_no2_model_name})')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "if hasattr(best_o3_model, 'feature_importances_'):\n",
    "    print(f\"Feature importance for {best_o3_model_name} (O3):\")\n",
    "    evaluator.plot_feature_importance(\n",
    "        best_o3_model, feature_names, \n",
    "        model_name=f\"{best_o3_model_name} (O3)\", \n",
    "        top_k=15\n",
    "    )\n",
    "\n",
    "if hasattr(best_no2_model, 'feature_importances_'):\n",
    "    print(f\"Feature importance for {best_no2_model_name} (NO2):\")\n",
    "    evaluator.plot_feature_importance(\n",
    "        best_no2_model, feature_names, \n",
    "        model_name=f\"{best_no2_model_name} (NO2)\", \n",
    "        top_k=15\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data for forecasting\n",
    "test_data_dict = data_loader.load_test_data()\n",
    "print(f\"Loaded test data for {len(test_data_dict)} sites\")\n",
    "\n",
    "# Apply same preprocessing and feature engineering to test data\n",
    "processed_test_data = {}\n",
    "\n",
    "for site_id, data in test_data_dict.items():\n",
    "    print(f\"Processing test data for Site {site_id}...\")\n",
    "    \n",
    "    # Clean and engineer features\n",
    "    cleaned_data = data_loader.clean_data(data)\n",
    "    features_df, _ = data_loader.prepare_features_targets(cleaned_data)\n",
    "    engineered_data = feature_engineer.create_all_features(features_df)\n",
    "    \n",
    "    processed_test_data[site_id] = engineered_data\n",
    "    print(f\"  Processed shape: {engineered_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize forecaster\n",
    "forecaster = AirQualityForecaster(config_path=str(config_path))\n",
    "\n",
    "# Load best models for forecasting\n",
    "best_models = {\n",
    "    'O3_' + best_o3_model_name: best_o3_model,\n",
    "    'NO2_' + best_no2_model_name: best_no2_model\n",
    "}\n",
    "\n",
    "# Load scalers if available\n",
    "scalers = model_trainer.scalers if hasattr(model_trainer, 'scalers') else {}\n",
    "\n",
    "forecaster.load_models(best_models, scalers, feature_names)\n",
    "print(f\"Forecaster status: {forecaster.get_forecast_status()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate forecasts for Site 1 as example\n",
    "site_id = 1\n",
    "site_test_data = processed_test_data[site_id]\n",
    "\n",
    "print(f\"Generating forecasts for Site {site_id}...\")\n",
    "\n",
    "# Generate 24-hour forecasts\n",
    "site_forecasts = forecaster.forecast_for_site(\n",
    "    site_test_data, \n",
    "    site_id,\n",
    "    target_columns=['O3_target', 'NO2_target'],\n",
    "    forecast_hours=24\n",
    ")\n",
    "\n",
    "print(f\"Generated forecasts for {len(site_forecasts)} targets\")\n",
    "for target, forecast_df in site_forecasts.items():\n",
    "    print(f\"  {target}: {len(forecast_df)} hourly predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display forecast results\n",
    "print(\"Sample O3 forecasts (first 10 hours):\")\n",
    "if 'O3_target' in site_forecasts:\n",
    "    display(site_forecasts['O3_target'].head(10))\n",
    "\n",
    "print(\"\\nSample NO2 forecasts (first 10 hours):\")\n",
    "if 'NO2_target' in site_forecasts:\n",
    "    display(site_forecasts['NO2_target'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize forecasts\n",
    "if site_forecasts:\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "    \n",
    "    # O3 forecast\n",
    "    if 'O3_target' in site_forecasts:\n",
    "        o3_forecast = site_forecasts['O3_target']\n",
    "        axes[0].plot(o3_forecast['datetime'], o3_forecast['O3_target_prediction'], \n",
    "                    label='O3 Prediction', linewidth=2)\n",
    "        axes[0].fill_between(o3_forecast['datetime'], \n",
    "                           o3_forecast['O3_target_lower_ci'], \n",
    "                           o3_forecast['O3_target_upper_ci'], \n",
    "                           alpha=0.3, label='90% Confidence Interval')\n",
    "        axes[0].set_title(f'O3 Forecast for Site {site_id} (24 hours)')\n",
    "        axes[0].set_ylabel('O3 (μg/m³)')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # NO2 forecast\n",
    "    if 'NO2_target' in site_forecasts:\n",
    "        no2_forecast = site_forecasts['NO2_target']\n",
    "        axes[1].plot(no2_forecast['datetime'], no2_forecast['NO2_target_prediction'], \n",
    "                    label='NO2 Prediction', linewidth=2, color='orange')\n",
    "        axes[1].fill_between(no2_forecast['datetime'], \n",
    "                           no2_forecast['NO2_target_lower_ci'], \n",
    "                           no2_forecast['NO2_target_upper_ci'], \n",
    "                           alpha=0.3, label='90% Confidence Interval', color='orange')\n",
    "        axes[1].set_title(f'NO2 Forecast for Site {site_id} (24 hours)')\n",
    "        axes[1].set_ylabel('NO2 (μg/m³)')\n",
    "        axes[1].set_xlabel('DateTime')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directories\n",
    "results_dir = project_root / 'results'\n",
    "models_dir = project_root / 'models'\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model comparison results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "o3_comparison.to_csv(results_dir / f'o3_model_comparison_{timestamp}.csv', index=False)\n",
    "no2_comparison.to_csv(results_dir / f'no2_model_comparison_{timestamp}.csv', index=False)\n",
    "\n",
    "print(f\"Model comparison results saved to {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best models\n",
    "model_trainer.save_model(best_o3_model_name, str(models_dir / f'best_o3_model_{timestamp}'))\n",
    "model_trainer.save_model(best_no2_model_name, str(models_dir / f'best_no2_model_{timestamp}'))\n",
    "\n",
    "print(f\"Best models saved to {models_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save forecasts\n",
    "if site_forecasts:\n",
    "    forecaster.save_forecasts(\n",
    "        site_forecasts,\n",
    "        output_dir=str(results_dir / 'forecasts'),\n",
    "        timestamp=timestamp\n",
    "    )\n",
    "    print(f\"Forecasts saved to {results_dir / 'forecasts'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive evaluation report\n",
    "o3_report = evaluator.create_comprehensive_report(\n",
    "    o3_comparison, \n",
    "    target_name=\"Ground-level O3\",\n",
    "    save_path=str(results_dir / f'o3_evaluation_report_{timestamp}.md')\n",
    ")\n",
    "\n",
    "no2_report = evaluator.create_comprehensive_report(\n",
    "    no2_comparison, \n",
    "    target_name=\"Ground-level NO2\",\n",
    "    save_path=str(results_dir / f'no2_evaluation_report_{timestamp}.md')\n",
    ")\n",
    "\n",
    "print(\"Comprehensive evaluation reports generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"AIR QUALITY FORECASTING - WORKFLOW SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. DATA PROCESSING:\")\n",
    "print(f\"   - Sites processed: {len(train_data_dict)}\")\n",
    "print(f\"   - Training samples: {len(all_sites_data):,}\")\n",
    "print(f\"   - Final features: {len(feature_names):,}\")\n",
    "print(f\"   - Date range: {all_sites_data['datetime'].min().strftime('%Y-%m-%d')} to {all_sites_data['datetime'].max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\n2. MODEL PERFORMANCE:\")\n",
    "print(f\"   Best O3 Model: {best_o3_model_name}\")\n",
    "print(f\"   - R²: {o3_comparison.iloc[0]['r2']:.4f}\")\n",
    "print(f\"   - RMSE: {o3_comparison.iloc[0]['rmse']:.4f}\")\n",
    "print(f\"   - MAE: {o3_comparison.iloc[0]['mae']:.4f}\")\n",
    "\n",
    "print(f\"   Best NO2 Model: {best_no2_model_name}\")\n",
    "print(f\"   - R²: {no2_comparison.iloc[0]['r2']:.4f}\")\n",
    "print(f\"   - RMSE: {no2_comparison.iloc[0]['rmse']:.4f}\")\n",
    "print(f\"   - MAE: {no2_comparison.iloc[0]['mae']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. FORECASTING CAPABILITY:\")\n",
    "print(f\"   - Forecast horizon: 24-48 hours\")\n",
    "print(f\"   - Prediction interval: Hourly\")\n",
    "print(f\"   - Confidence intervals: 90%\")\n",
    "print(f\"   - Target pollutants: O3, NO2\")\n",
    "\n",
    "print(f\"\\n4. OUTPUT FILES:\")\n",
    "print(f\"   - Model comparisons: {results_dir}\")\n",
    "print(f\"   - Trained models: {models_dir}\")\n",
    "print(f\"   - Forecasts: {results_dir / 'forecasts'}\")\n",
    "print(f\"   - Evaluation reports: {results_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WORKFLOW COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "logger.info(\"Air quality forecasting workflow completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
